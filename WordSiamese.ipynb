{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2d3911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]\n",
      "python: 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]\n",
      "torch: 1.12.1\n",
      "torchvision: 0.13.1\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from path import Path\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "\n",
    "print('python: ' + str(sys.version))\n",
    "print('torch: ' + str(torch.__version__))\n",
    "print('torchvision: ' + str(torchvision.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f57916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Paramaters\n",
    "iterations = 30000\n",
    "batch_size = 10 #90\n",
    "learning_rate = 0.0001 #0.00006\n",
    "way = 20\n",
    "caching = False\n",
    "modelPath = 'data/saved/finalmodels'\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomAffine(15),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "net_input_size = (448, 448)\n",
    "net_output_size = (224, 224)\n",
    "data_dir = Path('./wordsiamese/data/')\n",
    "minimum_data_samples = 5 #minimum number of samples of a word for that word to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "514d828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets and Loading\n",
    "\n",
    "class WordSet(Dataset):\n",
    "    def __init__(self, path, testing=False, transform = None, way=None, num_tests=None):\n",
    "        super(WordSet, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.images, self.class_count = self.loadTTImages('./wordsiamese/data/croppedImages')\n",
    "        self.testing=testing\n",
    "        self.num_tests = num_tests\n",
    "        self.way = way\n",
    "        self.classA, self.imgA = None, None #allows for n-way learning\n",
    "        \n",
    "    def loadTTImages(self, path):\n",
    "        images = {} #stores all images loaded with identical character under one key\n",
    "        idCount = 0 #number of different character types\n",
    "        for word in os.listdir(path): #language character comes from\n",
    "            word = str(word)\n",
    "            images[word] = []\n",
    "            for individualTest in os.listdir(os.path.join(path, word)): #each character image\n",
    "                f_path = os.path.join(path, word, individualTest)\n",
    "                #images[idCount].append(Image.open(f_path))\n",
    "                images[word].append(f_path)\n",
    "                idCount += 1\n",
    "        return images, idCount\n",
    "    \n",
    "    def loadData(self, path):\n",
    "        images = {} #stores all images loaded with identical character under one key\n",
    "        idCount = 0 #number of different character types\n",
    "        \n",
    "        gt_dir = path / 'gt'\n",
    "        img_dir = path / 'img'\n",
    "        \n",
    "        for gt in sorted(gt_dir.files('*.xml')):\n",
    "            img = img_dir / gt.stem + '.png'\n",
    "            if not img.exists():\n",
    "                continue\n",
    "            \n",
    "            tree = ET.parse(gt)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # go over all lines\n",
    "            for line in root.findall(\"./handwritten-part/line\"):\n",
    "\n",
    "                # go over all words\n",
    "                for word in line.findall('./word'):\n",
    "                    xmin, xmax, ymin, ymax = float('inf'), 0, float('inf'), 0\n",
    "                    success = False\n",
    "                    \n",
    "                    id_word = word.attrib['text'].lower()\n",
    "                    if id_word == '\\\"':\n",
    "                        id_word = 'double_quote_mark'\n",
    "                    elif id_word == '#':\n",
    "                        id_word = 'pound_mark'\n",
    "                    elif id_word == '&':\n",
    "                        id_word = 'ampersand'\n",
    "                    elif id_word == '{':\n",
    "                        id_word = 'left_curly_mark'\n",
    "                    elif id_word == '}':\n",
    "                        id_word = 'right_curly_mark'\n",
    "                    elif id_word == '*':\n",
    "                        id_word = 'asterisk_mark'\n",
    "                    elif id_word == '$':\n",
    "                        id_word = 'dollar_mark'\n",
    "                    elif id_word == '\\'':\n",
    "                        id_word = 'single_quote_mark'\n",
    "                    elif id_word == ':':\n",
    "                        id_word = 'colon_mark'\n",
    "                    elif id_word == '!':\n",
    "                        id_word = 'exclamation_mark'\n",
    "                    elif id_word == '?':\n",
    "                        id_word = 'question_mark'\n",
    "                    elif id_word == '.':\n",
    "                        id_word = 'period_mark'\n",
    "                    elif '.' in id_word or '\\\"' in id_word:\n",
    "                        continue\n",
    "                        \n",
    "                    # go over all characters\n",
    "                    for cmp in word.findall('./cmp'):\n",
    "                        success = True\n",
    "                        x = float(cmp.attrib['x'])\n",
    "                        y = float(cmp.attrib['y'])\n",
    "                        w = float(cmp.attrib['width'])\n",
    "                        h = float(cmp.attrib['height'])\n",
    "\n",
    "                        # aabb around all characters is aabb around word\n",
    "                        xmin = min(xmin, x)\n",
    "                        xmax = max(xmax, x + w)\n",
    "                        ymin = min(ymin, y)\n",
    "                        ymax = max(ymax, y + h)\n",
    "\n",
    "                    if success:\n",
    "                        if not id_word in images.keys():\n",
    "                            idCount += 1\n",
    "                            images[id_word] = []\n",
    "                        images[id_word].append(((xmin, ymin, xmax, ymax), img))\n",
    "        \n",
    "        remove = []\n",
    "        for key in images.keys():\n",
    "                if len(images[key]) < minimum_data_samples:\n",
    "                    remove.append(key)\n",
    "        \n",
    "        for r in remove:\n",
    "            images.pop(r)\n",
    "            \n",
    "        \n",
    "        return images, idCount\n",
    "    \n",
    "    def saveImages(self):\n",
    "        count = 0\n",
    "        for key in self.images.keys():\n",
    "            count += 1\n",
    "            os.mkdir(f'./wordsiamese/data/croppedImages/{key}/')\n",
    "            i = 0\n",
    "            shuffled_key = random.sample(self.images[key], len(self.images[key]))\n",
    "            for img in shuffled_key:\n",
    "                \n",
    "                img_box = img[0]\n",
    "                img_img = img[1]\n",
    "\n",
    "                # Opens a image in RGB mode\n",
    "                img_img = Image.open(img_img)\n",
    "                img_crop = img_img.crop(img_box).convert('L').resize((105, 105))                \n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    img_crop.save(f\"./wordsiamese/data/croppedImages/{key}/{i:04d}.jpg\")\n",
    "                except:\n",
    "                    print(count, key)\n",
    "                    print('saving exception failed due to complicated keys')\n",
    "                i += 1\n",
    "                if i > 10:\n",
    "                    break\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        label, imgA, imgB = None, None, None\n",
    "        \n",
    "        # i ensures that theres a mix of same and different sets\n",
    "        if self.testing:\n",
    "            if i % self.way == 0:\n",
    "                self.classA = random.choice(list(self.images.keys()))\n",
    "                self.imgA = random.choice(self.images[self.classA])\n",
    "                imgB = random.choice(self.images[self.classA])\n",
    "\n",
    "            #different class\n",
    "            else:\n",
    "                idClassB = random.choice(list(self.images.keys()))#set B every time but A only n-way times\n",
    "                while self.classA == idClassB: #prevents same class\n",
    "                    idClassB = random.choice(list(self.images.keys()))\n",
    "                imgB = random.choice(self.images[idClassB])\n",
    "            imgA = self.imgA\n",
    "            \n",
    "            label = label = torch.from_numpy(np.array([-1.00], dtype=np.float32))\n",
    "            \n",
    "        else:\n",
    "            #same class\n",
    "            if i % 2 == 1:\n",
    "                label = torch.from_numpy(np.array([1.00], dtype=np.float32))\n",
    "                idClass = random.choice(list(self.images.keys()))\n",
    "                imgA, imgB = random.choice(self.images[idClass]), random.choice(self.images[idClass])\n",
    "                while imgB == imgA:\n",
    "                    imgB = random.choice(self.images[idClass])\n",
    "\n",
    "            #different class\n",
    "            else:\n",
    "                label = torch.from_numpy(np.array([0.00], dtype=np.float32))\n",
    "                idClassA, idClassB = random.choice(list(self.images.keys())), random.choice(list(self.images.keys()))\n",
    "                while idClassA == idClassB: #prevents same class\n",
    "                    idClassB = random.choice(list(self.images.keys()))\n",
    "                imgA = random.choice(self.images[idClassA])\n",
    "                imgB = random.choice(self.images[idClassB])\n",
    "    \n",
    "        \n",
    "        # Opens a image in RGB mode\n",
    "        imgA = Image.open(imgA).convert('L')\n",
    "        imgB = Image.open(imgB).convert('L')\n",
    "        \n",
    "        imgA = self.transform(imgA)\n",
    "        imgB = self.transform(imgB)\n",
    "        \n",
    "        \n",
    "        return imgA, imgB, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.testing:\n",
    "            return self.num_tests * self.way\n",
    "        else:\n",
    "            return  21000000 #5250000\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3fd8066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data preperations\n",
    "\n",
    "trainSet = WordSet(data_dir, transform=data_transforms)\n",
    "#trainSet.saveImages()\n",
    "testSet = WordSet(data_dir, transform=transforms.ToTensor(), testing=True, num_tests = 400, way = 20)\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "testLoader = DataLoader(testSet, batch_size=way, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f57e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 - NVIDIA GeForce GTX 1650 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Check GPU\n",
    "device = torch.device(\"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\")\n",
    "print((str(device) + \" - \" + str(torch.cuda.get_device_name(torch.cuda.current_device()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6afc0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (10): ReLU()\n",
      "  )\n",
      "  (liner): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (out): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Siamese Model\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),  # 64@96*96\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64@48*48\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.ReLU(),    # 128@42*42\n",
    "            nn.MaxPool2d(2),   # 128@21*21\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.ReLU(), # 128@18*18\n",
    "            nn.MaxPool2d(2), # 128@9*9\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.ReLU(),   # 256@6*6\n",
    "        )\n",
    "        self.liner = nn.Sequential(nn.Linear(9216, 4096), nn.Sigmoid())\n",
    "        self.out = nn.Linear(4096, 1)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.liner(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        out = self.out(dis)\n",
    "        #return self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    net = Siamese()\n",
    "    print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing training\n",
      "starting training loop\n",
      "10 loss: 0.6928066432476043 time elapsed: 0.9025571346282959\n",
      "20 loss: 0.692585963010788 time elapsed: 0.7241277694702148\n",
      "30 loss: 0.6929052412509918 time elapsed: 0.712700605392456\n",
      "40 loss: 0.6927265048027038 time elapsed: 0.7139272689819336\n",
      "50 loss: 0.6888255119323731 time elapsed: 0.7139370441436768\n",
      "60 loss: 0.6614190101623535 time elapsed: 0.7131333351135254\n",
      "70 loss: 0.6504264593124389 time elapsed: 0.7121870517730713\n",
      "80 loss: 0.7036821603775024 time elapsed: 0.7123889923095703\n",
      "90 loss: 0.6751142263412475 time elapsed: 0.7129950523376465\n",
      "100 loss: 0.6718867063522339 time elapsed: 0.7117176055908203\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "400 Testing Set Correct: 40 Wrong: 360 Precision: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "110 loss: 0.6688785433769227 time elapsed: 18.0474591255188\n",
      "120 loss: 0.6904112040996552 time elapsed: 0.7100651264190674\n",
      "130 loss: 0.6628017008304596 time elapsed: 0.7150380611419678\n",
      "140 loss: 0.6502508819103241 time elapsed: 0.7115559577941895\n",
      "150 loss: 0.6256707608699799 time elapsed: 0.7154242992401123\n",
      "160 loss: 0.7014827251434326 time elapsed: 0.7204012870788574\n",
      "170 loss: 0.655367374420166 time elapsed: 0.7151157855987549\n",
      "180 loss: 0.6878284931182861 time elapsed: 0.7145733833312988\n",
      "190 loss: 0.6847854554653168 time elapsed: 0.7125129699707031\n",
      "200 loss: 0.6867069840431214 time elapsed: 0.7118315696716309\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n",
      "executing\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "print('initializing training')\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()#default for size average is true\n",
    "loss_value = 0\n",
    "loss_values = []\n",
    "\n",
    "network = Siamese() #creates a new network\n",
    "network.to(device)\n",
    "network.train() #sets the mode of the network to training\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "optimizer.zero_grad() #zeros out the gradients\n",
    "\n",
    "accuracies = []\n",
    "a_assist = []\n",
    "\n",
    "start_time = time.time()\n",
    "initial_start_time = start_time\n",
    "print('starting training loop')\n",
    "for i, (imgA, imgB, label) in enumerate(trainLoader, start=1):\n",
    "    if i > iterations:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "    imgA = Variable(imgA.cuda())\n",
    "    imgB = Variable(imgB.cuda())\n",
    "    label = Variable(label.cuda())\n",
    "    \n",
    "    optimizer.zero_grad() #zeros out the gradients since paramaters already updated with old gradient\n",
    "    \n",
    "    output = network.forward(imgA, imgB) #gets similarity probability\n",
    "    \n",
    "    loss = loss_function(output, label)\n",
    "    \n",
    "    li = loss.item()\n",
    "    \n",
    "    loss_value += li\n",
    "    loss_values.append(li)\n",
    "    loss.backward() #computes the gradient of loss for all parameters\n",
    "    \n",
    "    optimizer.step() #updates parameters\n",
    "    \n",
    "    #print updates for the user\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i} loss: {loss_value/10} time elapsed: {time.time()-start_time}')\n",
    "        loss_value = 0\n",
    "        start_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            correct, wrong = 0, 0\n",
    "            for i, (testA, testB, _) in enumerate(testLoader, 1):\n",
    "                testA, testB = Variable(testA.cuda()), Variable(testB.cuda())\n",
    "                output = network.forward(testA, testB).data.cpu().numpy() #computes the probability\n",
    "                prediction = np.argmax(output) #gets the index of highest value in output\n",
    "                if prediction == 0:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "\n",
    "            print('-'*100)\n",
    "            print(f'{i} Testing Set Correct: {correct} Wrong: {wrong} Precision: {correct*1.0/(correct + wrong)}')\n",
    "            print('-'*100)\n",
    "            accuracies.append(correct*1.0/(correct+wrong))\n",
    "            a_assist.append(i)\n",
    "            if i % 500 == 0:\n",
    "                torch.save(network.state_dict(), f'{modelPath}/training-model-{i}.pt')\n",
    "\n",
    "\n",
    "print('finish training loop, time elapsed: ', str(time.time()-initial_start_time))\n",
    "    \n",
    "#add final accuracies\n",
    "accuracy = 0.0\n",
    "counter = 0\n",
    "for d in accuracies:\n",
    "    print(d)\n",
    "    accuracy += d\n",
    "    counter += 1\n",
    "print(\"#\"*100)\n",
    "print(\"final accuracy: \", accuracy/counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15,20))\n",
    "#fig.tight_layout()\n",
    "\n",
    "#create subplots\n",
    "ax[0].plot(range(1, iterations + 1), loss_values, color='red')\n",
    "ax[0].set_title('Loss Values During Training')\n",
    "ax[0].set_ylabel('Loss Value')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[1].plot(a_assist, accuracies, color='blue')\n",
    "ax[1].set_title('Accuracies During Training')\n",
    "ax[1].set_ylabel('Accuracies')\n",
    "ax[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe77e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Image Loading\n",
    "imgT=mpimg.imread('./wordsiamese/data/img/a01-000u.png')\n",
    "plt.imshow(imgT)\n",
    "\n",
    "im = Image.open('./wordsiamese/data/img/a01-000u.png')\n",
    " \n",
    "newsize = (800, 300)\n",
    "im1 = im.resize\n",
    "# Shows the image in image viewer\n",
    "im.show()\n",
    "im1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb8ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research]",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
