{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2d3911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]\n",
      "python: 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]\n",
      "torch: 1.12.1\n",
      "torchvision: 0.13.1\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from path import Path\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "\n",
    "print('python: ' + str(sys.version))\n",
    "print('torch: ' + str(torch.__version__))\n",
    "print('torchvision: ' + str(torchvision.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f57916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Paramaters\n",
    "iterations = 30000\n",
    "batch_size = 10 #90\n",
    "learning_rate = 0.0001 #0.00006\n",
    "way = 20\n",
    "caching = False\n",
    "modelPath = 'data/saved/finalmodels'\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomAffine(15),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "net_input_size = (448, 448)\n",
    "net_output_size = (224, 224)\n",
    "data_dir = Path('./wordsiamese/data/')\n",
    "minimum_data_samples = 5 #minimum number of samples of a word for that word to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514d828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets and Loading\n",
    "\n",
    "class WordSet(Dataset):\n",
    "    def __init__(self, path, testing=False, transform = None, way=None, num_tests=None):\n",
    "        super(WordSet, self).__init__()\n",
    "        self.seed = 1\n",
    "        np.random.seed(self.seed)\n",
    "        self.transform = transform\n",
    "        self.images, self.class_count = self.loadData(path)\n",
    "        self.testing=testing\n",
    "        self.num_tests = num_tests\n",
    "        self.way = way\n",
    "        self.classA, self.imgA = None, None #allows for n-way learning\n",
    "        \n",
    "    def loadData(self, path):\n",
    "        images = {} #stores all images loaded with identical character under one key\n",
    "        idCount = 0 #number of different character types\n",
    "        \n",
    "        gt_dir = path / 'gt'\n",
    "        img_dir = path / 'img'\n",
    "        \n",
    "        for gt in sorted(gt_dir.files('*.xml')):\n",
    "            img = img_dir / gt.stem + '.png'\n",
    "            if not img.exists():\n",
    "                continue\n",
    "            \n",
    "            tree = ET.parse(gt)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # go over all lines\n",
    "            for line in root.findall(\"./handwritten-part/line\"):\n",
    "\n",
    "                # go over all words\n",
    "                for word in line.findall('./word'):\n",
    "                    xmin, xmax, ymin, ymax = float('inf'), 0, float('inf'), 0\n",
    "                    success = False\n",
    "                    \n",
    "                    id_word = word.attrib['text'].lower()\n",
    "\n",
    "                    # go over all characters\n",
    "                    for cmp in word.findall('./cmp'):\n",
    "                        success = True\n",
    "                        x = float(cmp.attrib['x'])\n",
    "                        y = float(cmp.attrib['y'])\n",
    "                        w = float(cmp.attrib['width'])\n",
    "                        h = float(cmp.attrib['height'])\n",
    "\n",
    "                        # aabb around all characters is aabb around word\n",
    "                        xmin = min(xmin, x)\n",
    "                        xmax = max(xmax, x + w)\n",
    "                        ymin = min(ymin, y)\n",
    "                        ymax = max(ymax, y + h)\n",
    "\n",
    "                    if success:\n",
    "                        if not id_word in images.keys():\n",
    "                            idCount += 1\n",
    "                            images[id_word] = []\n",
    "                        images[id_word].append(((xmin, ymin, xmax, ymax), img))\n",
    "        \n",
    "        remove = []\n",
    "        for key in images.keys():\n",
    "                if len(images[key]) < minimum_data_samples:\n",
    "                    remove.append(key)\n",
    "        \n",
    "        for r in remove:\n",
    "            images.pop(r)\n",
    "        \n",
    "        return images, idCount\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        label, imgA, imgB = None, None, None\n",
    "        \n",
    "        # i ensures that theres a mix of same and different sets\n",
    "        if self.testing:\n",
    "            if i % self.way == 0:\n",
    "                self.classA = random.randint(0, self.class_count - 1)\n",
    "                self.imgA = random.choice(self.images[self.classA])\n",
    "                imgB = random.choice(self.images[self.classA])\n",
    "\n",
    "            #different class\n",
    "            else:\n",
    "                idClassB = random.randint(0, self.class_count - 1) #set B every time but A only n-way times\n",
    "                while self.classA == idClassB: #prevents same class\n",
    "                    idClassB = random.randint(0, self.class_count - 1)\n",
    "                imgB = random.choice(self.images[idClassB])\n",
    "            imgA = self.imgA\n",
    "            \n",
    "        else:\n",
    "            #same class\n",
    "            if i % 2 == 1:\n",
    "                label = torch.from_numpy(np.array([1.00], dtype=np.float32))\n",
    "                idClass = random.choice(list(self.images.keys()))\n",
    "                imgA, imgB = random.choice(self.images[idClass]), random.choice(self.images[idClass])\n",
    "                while imgB == imgA:\n",
    "                    imgB = random.choice(self.images[idClass])\n",
    "\n",
    "            #different class\n",
    "            else:\n",
    "                label = torch.from_numpy(np.array([0.00], dtype=np.float32))\n",
    "                idClassA, idClassB = random.choice(list(self.images.keys())), random.choice(list(self.images.keys()))\n",
    "                while idClassA == idClassB: #prevents same class\n",
    "                    idClassB = random.choice(list(self.images.keys()))\n",
    "                imgA = random.choice(self.images[idClassA])\n",
    "                imgB = random.choice(self.images[idClassB])\n",
    "        \n",
    "        imgA_box = imgA[0]\n",
    "        imgA_img = imgA[1]\n",
    "        \n",
    "        imgB_box = imgB[0]\n",
    "        imgB_img = imgB[1]\n",
    "        \n",
    "        # Opens a image in RGB mode\n",
    "        imgA_img = Image.open(imgA_img)\n",
    "        imgA_crop = imgA_img.crop(imgA_box).convert('L')\n",
    "        \n",
    "        imgB_img = Image.open(imgB_img)\n",
    "        imgB_crop = imgB_img.crop(imgB_box).convert('L')\n",
    "        \n",
    "        imgA = self.transform(imgA_crop)\n",
    "        imgB = self.transform(imgB_crop)\n",
    "        \n",
    "        \n",
    "        return imgA, imgB, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.testing:\n",
    "            return self.num_tests * self.way\n",
    "        else:\n",
    "            return  21000000 #5250000\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fd8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preperations\n",
    "\n",
    "trainSet = WordSet(data_dir, transform=data_transforms)\n",
    "testSet = WordSet(data_dir, transform=transforms.ToTensor(), num_tests = 400, way = 20)\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "testLoader = DataLoader(testSet, batch_size=way, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f57e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 - NVIDIA GeForce GTX 1650 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Check GPU\n",
    "device = torch.device(\"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\")\n",
    "print((str(device) + \" - \" + str(torch.cuda.get_device_name(torch.cuda.current_device()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (10): ReLU()\n",
      "  )\n",
      "  (liner): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (out): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Siamese Model\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),  # 64@96*96\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64@48*48\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.ReLU(),    # 128@42*42\n",
    "            nn.MaxPool2d(2),   # 128@21*21\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.ReLU(), # 128@18*18\n",
    "            nn.MaxPool2d(2), # 128@9*9\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.ReLU(),   # 256@6*6\n",
    "        )\n",
    "        self.liner = nn.Sequential(nn.Linear(9216, 4096), nn.Sigmoid())\n",
    "        self.out = nn.Linear(4096, 1)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.liner(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        out = self.out(dis)\n",
    "        #return self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    net = Siamese()\n",
    "    print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c1c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing training\n",
      "starting training loop\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 103, 290] at entry 0 and [1, 81, 58] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m initial_start_time \u001b[38;5;241m=\u001b[39m start_time\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training loop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (imgA, imgB, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainLoader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m iterations:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 103, 290] at entry 0 and [1, 81, 58] at entry 1"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "print('initializing training')\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()#default for size average is true\n",
    "loss_value = 0\n",
    "loss_values = []\n",
    "\n",
    "network = Siamese() #creates a new network\n",
    "network.to(device)\n",
    "network.train() #sets the mode of the network to training\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "optimizer.zero_grad() #zeros out the gradients\n",
    "\n",
    "accuracies = []\n",
    "a_assist = []\n",
    "\n",
    "start_time = time.time()\n",
    "initial_start_time = start_time\n",
    "print('starting training loop')\n",
    "for i, (imgA, imgB, label) in enumerate(trainLoader, start=1):\n",
    "    if i > iterations:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "    imgA = Variable(imgA.cuda())\n",
    "    imgB = Variable(imgB.cuda())\n",
    "    label = Variable(label.cuda())\n",
    "    \n",
    "    optimizer.zero_grad() #zeros out the gradients since paramaters already updated with old gradient\n",
    "    \n",
    "    output = network.forward(imgA, imgB) #gets similarity probability\n",
    "    \n",
    "    loss = loss_function(output, label)\n",
    "    \n",
    "    li = loss.item()\n",
    "    \n",
    "    loss_value += li\n",
    "    loss_values.append(li)\n",
    "    loss.backward() #computes the gradient of loss for all parameters\n",
    "    \n",
    "    optimizer.step() #updates parameters\n",
    "    \n",
    "    #print updates for the user\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i} loss: {loss_value/10} time elapsed: {time.time()-start_time}')\n",
    "        loss_value = 0\n",
    "        start_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            correct, wrong = 0, 0\n",
    "            for _, (testA, testB, _) in enumerate(testLoader, 1):\n",
    "\n",
    "                testA, testB = Variable(testA.cuda()), Variable(testB.cuda())\n",
    "                output = network.forward(testA, testB).data.cpu().numpy() #computes the probability\n",
    "                prediction = np.argmax(output) #gets the index of highest value in output\n",
    "                if prediction == 0:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "\n",
    "            print('-'*100)\n",
    "            print(f'{i} Testing Set Correct: {correct} Wrong: {wrong} Precision: {correct*1.0/(correct + wrong)}')\n",
    "            print('-'*100)\n",
    "            accuracies.append(correct*1.0/(correct+wrong))\n",
    "            a_assist.append(i)\n",
    "            if i % 500 == 0:\n",
    "                torch.save(network.state_dict(), f'{modelPath}/training-model-{i}.pt')\n",
    "\n",
    "\n",
    "print('finish training loop, time elapsed: ', str(time.time()-initial_start_time))\n",
    "    \n",
    "#add final accuracies\n",
    "accuracy = 0.0\n",
    "counter = 0\n",
    "for d in accuracies:\n",
    "    print(d)\n",
    "    accuracy += d\n",
    "    counter += 1\n",
    "print(\"#\"*100)\n",
    "print(\"final accuracy: \", accuracy/counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15,20))\n",
    "#fig.tight_layout()\n",
    "\n",
    "#create subplots\n",
    "ax[0].plot(range(1, iterations + 1), loss_values, color='red')\n",
    "ax[0].set_title('Loss Values During Training')\n",
    "ax[0].set_ylabel('Loss Value')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[1].plot(a_assist, accuracies, color='blue')\n",
    "ax[1].set_title('Accuracies During Training')\n",
    "ax[1].set_ylabel('Accuracies')\n",
    "ax[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe77e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Image Loading\n",
    "imgT=mpimg.imread('./wordsiamese/data/img/a01-000u.png')\n",
    "plt.imshow(imgT)\n",
    "\n",
    "im = Image.open('./wordsiamese/data/img/a01-000u.png')\n",
    " \n",
    "# Size of the image in pixels (size of original image)\n",
    "# (This is not mandatory)\n",
    "width, height = im.size\n",
    " \n",
    "# Setting the points for cropped image\n",
    "left = 0\n",
    "top = 0\n",
    "right = width/2\n",
    "bottom = height/2\n",
    " \n",
    "# Cropped image of above dimension\n",
    "# (It will not change original image)\n",
    "im1 = im.crop((left, top, right, bottom))\n",
    " \n",
    "# Shows the image in image viewer\n",
    "im1.show()\n",
    "print(type(im1))\n",
    "print(im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb8ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research]",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
